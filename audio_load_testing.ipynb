{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from soundfile import read as sf_read\n",
    "from librosa import load as lib_load\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('./data/processed_genres.csv')\n",
    "tracks = np.array(tracks.loc[:, ['parent_genre_id', 'fpath']])\n",
    "\n",
    "dim = 44100 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IO>AudioReadableRead_device_/job:localhost/replica:0/task:0/device:CPU:0}} read 264600 from 1190107 failed: 0 [Op:IO>AudioReadableRead]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Peter\\Documents\\genre_classifier\\audio_load_testing.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Documents/genre_classifier/audio_load_testing.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m shape \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(audio\u001b[39m.\u001b[39mshape, tf\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Documents/genre_classifier/audio_load_testing.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m start_idx \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(shape\u001b[39m=\u001b[39m[], minval\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, maxval\u001b[39m=\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m dim, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Peter/Documents/genre_classifier/audio_load_testing.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m audio_slice \u001b[39m=\u001b[39m audio[start_idx:start_idx \u001b[39m+\u001b[39;49m dim]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Documents/genre_classifier/audio_load_testing.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Convert to one channel\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Documents/genre_classifier/audio_load_testing.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# either by averaging stereo channels or removing extra dim on mono\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Peter/Documents/genre_classifier/audio_load_testing.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mequal(shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m2\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow_io\\python\\ops\\audio_ops.py:735\u001b[0m, in \u001b[0;36mAudioIOTensor.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    732\u001b[0m start \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m e \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m indices[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    733\u001b[0m stop \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m e \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m indices[\u001b[39m1\u001b[39m]]\n\u001b[1;32m--> 735\u001b[0m item \u001b[39m=\u001b[39m core_ops\u001b[39m.\u001b[39;49mio_audio_readable_read(\n\u001b[0;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resource, start\u001b[39m=\u001b[39;49mstart, stop\u001b[39m=\u001b[39;49mstop, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype\n\u001b[0;32m    737\u001b[0m )\n\u001b[0;32m    739\u001b[0m \u001b[39m# in case certain dimension is not slice, then this dimension will need to\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39m# collapse as `0`, otherwise `:` or `slice(None, None, None)`\u001b[39;00m\n\u001b[0;32m    741\u001b[0m indices \u001b[39m=\u001b[39m [\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, \u001b[39mslice\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m key]\n",
      "File \u001b[1;32m<string>:2787\u001b[0m, in \u001b[0;36mio_audio_readable_read\u001b[1;34m(input, start, stop, dtype, name)\u001b[0m\n",
      "File \u001b[1;32m<string>:2839\u001b[0m, in \u001b[0;36mio_audio_readable_read_eager_fallback\u001b[1;34m(input, start, stop, dtype, name, ctx)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IO>AudioReadableRead_device_/job:localhost/replica:0/task:0/device:CPU:0}} read 264600 from 1190107 failed: 0 [Op:IO>AudioReadableRead]"
     ]
    }
   ],
   "source": [
    "# Pure tensorflow test\n",
    "for x in range(5000):\n",
    "    fpath = './data/fma_medium' + tracks[x][-1]\n",
    "    audio = tfio.audio.AudioIOTensor(fpath)\n",
    "    shape = tf.cast(audio.shape, tf.dtypes.int32)\n",
    "    start_idx = tf.random.uniform(shape=[], minval=0, maxval=shape[0] - dim, dtype=tf.dtypes.int32)\n",
    "    audio_slice = audio[start_idx:start_idx + dim]\n",
    "    # Convert to one channel\n",
    "    # either by averaging stereo channels or removing extra dim on mono\n",
    "    if tf.equal(shape[-1], 2):\n",
    "        audio_tensor = tf.reduce_mean(audio_slice, axis=[-1])\n",
    "    else:\n",
    "        audio_tensor = tf.squeeze(audio_slice, axis=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa test\n",
    "for x in range(5000):\n",
    "    fpath = './data/fma_medium' + tracks[x][-1]\n",
    "    audio = tfio.audio.AudioIOTensor(fpath)\n",
    "    shape = tf.cast(audio.shape, tf.dtypes.int32)\n",
    "    start_idx = tf.random.uniform(shape=[], minval=0, maxval=tf.cast(tf.divide(shape[0] - dim, 44100), tf.dtypes.float32), dtype=tf.dtypes.float32)\n",
    "    audio_slice, _ = lib_load(fpath, sr=44100, mono=True, offset=start_idx, duration=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soundfile test\n",
    "for x in range(5000):\n",
    "    fpath = './data/fma_medium' + tracks[x][-1]\n",
    "    audio = tfio.audio.AudioIOTensor(fpath)\n",
    "    shape = tf.cast(audio.shape, tf.dtypes.int32)\n",
    "    start_idx = tf.random.uniform(shape=[], minval=0, maxval=shape[0] - dim, dtype=tf.dtypes.int32)\n",
    "    audio_slice, _ = sf_read(fpath, frames=dim, start=start_idx)\n",
    "    # Convert to one channel by averaging stereo channels\n",
    "    if tf.equal(shape[-1], 2):\n",
    "        audio_tensor = tf.reduce_mean(audio_slice, axis=[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
