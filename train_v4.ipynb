{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline_v4 import DataGen\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the dataset from our generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "load_latest_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fixup_shape(x, y):\n",
    "  x.set_shape([None, 259, 128]) # n, h, w, c\n",
    "  y.set_shape([None]) # n, nb_classes\n",
    "  return x, y\n",
    "\n",
    "batch_size = 64\n",
    "tracks = pd.read_csv('./data/processed_genres_mel.csv')\n",
    "\n",
    "# Parse filepaths\n",
    "track_fpaths = list(tracks['fpath'])\n",
    "track_fpaths = ['./data/fma_medium' + fpath for fpath in track_fpaths]\n",
    "\n",
    "# Set up generator processing function\n",
    "gen = DataGen()\n",
    "\n",
    "# Set up train and test data\n",
    "data = (track_fpaths, list(tracks['parent_genre_id']))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.map(lambda fpath, label: tuple(tf.py_function(gen.get_sample, [fpath, label], [tf.float32, tf.int32])),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size=len(track_fpaths))\n",
    "\n",
    "# Define the split ratio for train/test datasets\n",
    "num_train_samples = int(0.8 * len(track_fpaths))\n",
    "num_test_samples = len(track_fpaths) - num_train_samples\n",
    "\n",
    "# Split into train and test datasets\n",
    "train_dataset = dataset.take(num_train_samples)\n",
    "test_dataset = dataset.skip(num_train_samples)\n",
    "\n",
    "train_dataset = train_dataset.repeat().batch(batch_size).map(_fixup_shape).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.repeat().batch(batch_size).map(_fixup_shape).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class weights for balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = np.array(tracks['parent_genre_id'])\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(genres),\n",
    "                                                  y=genres)\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add checkpoint callback for saving every few epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current saving/loading folder\n",
    "if load_latest_model:\n",
    "    training_dir_list = os.listdir('./training/')\n",
    "    training_dir_list.sort()\n",
    "    save_dir = './training/' + training_dir_list[-1]\n",
    "else:\n",
    "    dt_now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    save_dir = f'./training/training_{dt_now}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = save_dir + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "n_batches = num_train_samples // batch_size\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq=2*n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one for backups so we can continue training if interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_callback = tf.keras.callbacks.BackupAndRestore(\n",
    "    save_dir,\n",
    "    save_freq=\"epoch\",\n",
    "    delete_checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one for history logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = tf.keras.callbacks.CSVLogger(save_dir + '/history.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(259, dropout=0.2, recurrent_dropout=0.2), input_shape=(259, 128)),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "    # tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "    # tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "    # tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "    # tf.keras.layers.Dense(32, activation=\"relu\"),  \n",
    "    tf.keras.layers.Dense(16)\n",
    "])\n",
    "    \n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"acc\"], optimizer='adam')\n",
    "\n",
    "history = model.fit(x=train_dataset, epochs=num_epochs,\n",
    "                    validation_data=test_dataset, class_weight=class_weights,\n",
    "                    steps_per_epoch=num_train_samples // batch_size,\n",
    "                    validation_steps=num_test_samples // batch_size,\n",
    "                    callbacks=[backup_callback, cp_callback, csv_logger])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
