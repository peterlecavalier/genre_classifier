{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline_v4 import DataGen\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the dataset from our generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "# Whether or not to load a model. If False/0, create a new model.\n",
    "# If positive integer, load that most recent model (1=most recent, 2=second-most recent, etc.)\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fixup_shape(x, y):\n",
    "  x.set_shape([None, 259, 128]) # n, h, w, c\n",
    "  y.set_shape([None]) # n, nb_classes\n",
    "  return x, y\n",
    "\n",
    "batch_size = 64\n",
    "tracks = pd.read_csv('./data/processed_genres_mel.csv')\n",
    "\n",
    "# Parse filepaths\n",
    "track_fpaths = list(tracks['fpath'])\n",
    "track_fpaths = ['./data/fma_medium' + fpath for fpath in track_fpaths]\n",
    "\n",
    "# Set up generator processing function\n",
    "gen = DataGen()\n",
    "\n",
    "# Set up train and test data\n",
    "data = (track_fpaths, list(tracks['parent_genre_id']))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.map(lambda fpath, label: tuple(tf.py_function(gen.get_sample, [fpath, label], [tf.float32, tf.int32])),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size=len(track_fpaths), seed=1, reshuffle_each_iteration=False)\n",
    "\n",
    "# Define the split ratio for train/test datasets\n",
    "num_train_samples = int(0.8 * len(track_fpaths))\n",
    "num_test_samples = len(track_fpaths) - num_train_samples\n",
    "\n",
    "# Split into train and test datasets\n",
    "train_dataset = dataset.take(num_train_samples)\n",
    "test_dataset = dataset.skip(num_train_samples)\n",
    "\n",
    "train_dataset = train_dataset.repeat().shuffle(buffer_size=num_train_samples,\n",
    "                                               reshuffle_each_iteration=True).batch(batch_size).map(_fixup_shape).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.repeat().shuffle(buffer_size=num_test_samples,\n",
    "                                             reshuffle_each_iteration=True).batch(batch_size).map(_fixup_shape).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class weights for balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7109196713829302,\n",
       " 1: 1.3133431703204048,\n",
       " 2: 0.2196312746756909,\n",
       " 3: 1.0261034255599473,\n",
       " 4: 0.6935106856634016,\n",
       " 5: 4.056315104166667,\n",
       " 6: 0.2470068189026324,\n",
       " 7: 1.5498756218905472,\n",
       " 8: 10.114448051948052,\n",
       " 9: 21.048986486486488,\n",
       " 10: 13.20021186440678,\n",
       " 11: 8.750702247191011,\n",
       " 12: 2.5327235772357723,\n",
       " 13: 3.2049897119341564,\n",
       " 14: 1.1555081602373887,\n",
       " 15: 74.17261904761905}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = np.array(tracks['parent_genre_id'])\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(genres),\n",
    "                                                  y=genres)\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add checkpoint callback for saving every few epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current saving/loading folder\n",
    "if not load_model:\n",
    "    dt_now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    save_dir = f'./training/training_{dt_now}'\n",
    "elif load_model > 0 and isinstance(load_model, int):\n",
    "    training_dir_list = os.listdir('./training/')\n",
    "    training_dir_list.sort()\n",
    "    save_dir = './training/' + training_dir_list[-load_model]\n",
    "else:\n",
    "    raise ValueError(\"Incorrect model loading integer (see load_model var)\")\n",
    "\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = save_dir + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "n_batches = num_train_samples // batch_size\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq='epoch')#2*n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one for backups so we can continue training if interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_callback = tf.keras.callbacks.BackupAndRestore(\n",
    "    save_dir,\n",
    "    save_freq=\"epoch\",\n",
    "    delete_checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one for history logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = tf.keras.callbacks.CSVLogger(save_dir + '/history.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 600)              1029600   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 600)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                9616      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,039,216\n",
      "Trainable params: 1,039,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "311/311 [==============================] - 1488s 4s/step - loss: 2.7190 - acc: 0.1175 - val_loss: 2.6626 - val_acc: 0.0519\n",
      "Epoch 2/50\n",
      "311/311 [==============================] - 1338s 4s/step - loss: 2.5043 - acc: 0.2213 - val_loss: 2.5347 - val_acc: 0.1234\n",
      "Epoch 3/50\n",
      "311/311 [==============================] - 1313s 4s/step - loss: 2.4946 - acc: 0.2019 - val_loss: 2.4252 - val_acc: 0.2567\n",
      "Epoch 4/50\n",
      "311/311 [==============================] - 1334s 4s/step - loss: 2.4962 - acc: 0.2067 - val_loss: 2.5604 - val_acc: 0.0881\n",
      "Epoch 5/50\n",
      "311/311 [==============================] - 1327s 4s/step - loss: 2.3883 - acc: 0.2671 - val_loss: 2.7808 - val_acc: 0.1607\n",
      "Epoch 6/50\n",
      "107/311 [=========>....................] - ETA: 12:39 - loss: 2.4404 - acc: 0.1951"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    f = open(save_dir + '/model.json', \"r\")\n",
    "    config = f.read()\n",
    "    model = tf.keras.models.model_from_json(config)\n",
    "    f.close()\n",
    "else:\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2), input_shape=(259, 128)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        # tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        # tf.keras.layers.Dropout(0.2),\n",
    "        # tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        # tf.keras.layers.Dropout(0.2),\n",
    "        # tf.keras.layers.Dense(32, activation=\"relu\"),  \n",
    "        tf.keras.layers.Dense(16)\n",
    "    ])\n",
    "\n",
    "    f = open(save_dir + '/model.json', \"w\")\n",
    "    f.write(model.to_json())\n",
    "    f.close()\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"acc\"], optimizer='adam')\n",
    "\n",
    "history = model.fit(x=train_dataset, epochs=num_epochs,\n",
    "                    validation_data=test_dataset, class_weight=class_weights,\n",
    "                    steps_per_epoch=num_train_samples // batch_size,\n",
    "                    validation_steps=num_test_samples // batch_size,\n",
    "                    callbacks=[backup_callback, cp_callback, csv_logger])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
