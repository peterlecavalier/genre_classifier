{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline_new import DataGen\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the dataset from our generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19941\n",
      "WARNING:tensorflow:From C:\\Users\\Peter\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "def _fixup_shape(x, y):\n",
    "  x.set_shape([None, 512, 512]) # n, h, w, c\n",
    "  y.set_shape([None]) # n, nb_classes\n",
    "  return x, y\n",
    "\n",
    "batch_size = 16\n",
    "tracks = pd.read_csv('./data/processed_genres.csv')\n",
    "\n",
    "# Set up train/test split\n",
    "all_idxs = list(range(len(tracks)))\n",
    "np.random.shuffle(all_idxs)\n",
    "num_train = np.floor(len(all_idxs) * 0.8).astype(np.int32)\n",
    "train_idxs = all_idxs[:num_train]\n",
    "test_idxs = all_idxs[num_train:]\n",
    "\n",
    "# Set up generator processing function\n",
    "gen = DataGen(tracks, batch_size=batch_size)\n",
    "\n",
    "# Set up train data\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_idxs, tf.uint16)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_idxs),\n",
    "                                      seed=0, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.map(lambda i: tf.py_function(func=gen.get_sample, \n",
    "                                                           inp=[i], \n",
    "                                                           Tout=[tf.float32,\n",
    "                                                                 tf.int32]\n",
    "                                                          ), \n",
    "                                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.repeat(num_epochs)\n",
    "train_dataset = train_dataset.batch(batch_size).map(_fixup_shape).repeat()\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Set up test data\n",
    "test_dataset = tf.data.Dataset.from_generator(lambda: test_idxs, tf.uint16)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=len(test_idxs),\n",
    "                                      seed=0, reshuffle_each_iteration=True)\n",
    "test_dataset = test_dataset.map(lambda i: tf.py_function(func=gen.get_sample, \n",
    "                                                           inp=[i], \n",
    "                                                           Tout=[tf.float32,\n",
    "                                                                 tf.int32]\n",
    "                                                          ), \n",
    "                                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.repeat(num_epochs)\n",
    "test_dataset = test_dataset.batch(batch_size).map(_fixup_shape)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class weights for balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7104138166894665,\n",
       " 1: 1.3136066610455313,\n",
       " 2: 0.21967533840947548,\n",
       " 3: 1.0263092885375493,\n",
       " 4: 0.6930326957295374,\n",
       " 5: 4.05712890625,\n",
       " 6: 0.24705637488106566,\n",
       " 7: 1.550186567164179,\n",
       " 8: 10.116477272727273,\n",
       " 9: 21.05320945945946,\n",
       " 10: 13.202860169491526,\n",
       " 11: 8.75245786516854,\n",
       " 12: 2.533231707317073,\n",
       " 13: 3.205632716049383,\n",
       " 14: 1.1548832468495183,\n",
       " 15: 74.1875}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = np.array(tracks['parent_genre_id'])\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(genres),\n",
    "                                                  y=genres)\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 497, 497, 16)      4112      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 124, 124, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 124, 124, 16)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 32)      131104    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 27, 27, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 27, 27, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 23328)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                746528    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 882,272\n",
      "Trainable params: 882,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      " 151/1246 [==>...........................] - ETA: 1:42:44 - loss: 3.0680 - accuracy: 0.1602"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (16,16), input_shape=(512, 512, 1), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((4, 4)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(32, (16, 16), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((4, 4)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(16)\n",
    "])\n",
    "    \n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"], optimizer='adam')\n",
    "\n",
    "history = model.fit(x=train_dataset, epochs=50,\n",
    "                    validation_data=test_dataset, class_weight=class_weights,\n",
    "                    steps_per_epoch=len(train_idxs) // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
